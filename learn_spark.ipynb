{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-483ba59fe15a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "findspark.init() \n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml import feature, classification\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from typing import Iterator, BinaryIO, Tuple\n",
    "from pyspark.sql.types import FloatType, ArrayType, LongType\n",
    "from pyspark.sql.functions import col, split, udf, pandas_udf, PandasUDFType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fichiers nécessaires si pas déjà dans le répertoire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://s3-eu-west-1.amazonaws.com/course.oc-static.com/courses/4297166/agents.json\n",
    "#!wget http://classics.mit.edu/Homer/iliad.mb.txt\n",
    "#!wget http://classics.mit.edu/Homer/odyssey.mb.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiser une session Spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark tests\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commandes de base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## json et dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- country_name: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lire le json, voir la structure\n",
    "agents = spark.read.json(\"agents.json\")\n",
    "agents.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------------------+------------------+------+\n",
      "|country_name|        id|          latitude|         longitude|   sex|\n",
      "+------------+----------+------------------+------------------+------+\n",
      "|       China| 227417393| 33.15219798270325|100.85840672174572|  Male|\n",
      "|       Haiti|6821129477|19.325567983697297|-72.43795260265814|Female|\n",
      "|       India|2078667700|23.645271492037235| 80.85636526088884|Female|\n",
      "|       China| 477556555| 33.45864668881662| 93.33604038078953|Female|\n",
      "|       India|1379059984|28.816938290678692|  80.7728698035823|Female|\n",
      "+------------+----------+------------------+------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Voir les5 premières lignes\n",
    "agents.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------------------+------------------+------+\n",
      "|country_name|        id|          latitude|         longitude|   sex|\n",
      "+------------+----------+------------------+------------------+------+\n",
      "|       India|2078667700|23.645271492037235| 80.85636526088884|Female|\n",
      "|       China| 477556555| 33.45864668881662| 93.33604038078953|Female|\n",
      "|       India|1379059984|28.816938290678692|  80.7728698035823|Female|\n",
      "|       India|1375733494|22.385712662257426| 77.90320433636231|Female|\n",
      "|       China| 265404548|29.447948266668902|106.56441719305467|Female|\n",
      "+------------+----------+------------------+------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrer sur plusieurs critères\n",
    "agents.filter(agents.sex==\"Female\") \\\n",
    "    .filter(agents.latitude>20) \\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+-------------------+-------------------+------+\n",
      "|    country_name|        id|           latitude|          longitude|   sex|\n",
      "+----------------+----------+-------------------+-------------------+------+\n",
      "|French Polynesia|7170821229|-15.004219445056265|-140.01650828107668|  Male|\n",
      "|   United States|2663608288|  38.34123451380646|-115.14211841078837|  Male|\n",
      "|   United States|2651719771|  37.05792617608187|-109.88579232396816|Female|\n",
      "+----------------+----------+-------------------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ordonner, montrer les 3 premiers\n",
    "agents.orderBy(agents.longitude) \\\n",
    "    .limit(3) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de vue pour utiliser les requêtes SQL\n",
    "agents.createTempView(\"agents_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+-------------------+-------------------+------+\n",
      "|    country_name|        id|           latitude|          longitude|   sex|\n",
      "+----------------+----------+-------------------+-------------------+------+\n",
      "|French Polynesia|7170821229|-15.004219445056265|-140.01650828107668|  Male|\n",
      "|   United States|2663608288|  38.34123451380646|-115.14211841078837|  Male|\n",
      "|   United States|2651719771|  37.05792617608187|-109.88579232396816|Female|\n",
      "+----------------+----------+-------------------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM agents_view ORDER BY longitude LIMIT 3\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[34] at javaToPython at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertir en rdd\n",
    "agents.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(country_name='China', id=227417393, latitude=33.15219798270325, longitude=100.85840672174572, sex='Male')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Montrer la première ligne\n",
    "agents.rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China 227417393\n"
     ]
    }
   ],
   "source": [
    "first_row = agents.rdd.first()\n",
    "print(first_row.country_name, first_row.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(country_name='Ambre', id=0, latitude=0, longitude=0, sex='Undefined')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Création et ajout d'une nouvelle ligne au rdd\n",
    "pyspark.sql.Row(country_name=\"Ambre\", id=000, latitude=0, longitude=0, sex=\"Undefined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation d'un fichier texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation d'un ficier texte en rdd\n",
    "iliad_rdd = spark.sparkContext.textFile(\"iliad.mb.txt\") \\\n",
    "    .flatMap(lambda line: line.split()) \\\n",
    "    .map(lambda word: word.strip(\",;.?:/!\\\"'~()-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion des objets contenus dans le RDD en rows\n",
    "iliad_rows = iliad_rdd.map(lambda word: pyspark.sql.Row(word=word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[39] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iliad_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    word|\n",
      "+--------+\n",
      "|Provided|\n",
      "|      by|\n",
      "|     The|\n",
      "|Internet|\n",
      "|Classics|\n",
      "+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Création d'un df à partir des rows\n",
    "iliad = spark.createDataFrame(iliad_rows)\n",
    "iliad.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      word|\n",
      "+----------+\n",
      "|      zeal|\n",
      "|    youths|\n",
      "|     youth|\n",
      "| youselves|\n",
      "|yourselves|\n",
      "|  yourself|\n",
      "|     yours|\n",
      "|   yourelf|\n",
      "|      your|\n",
      "|  youngest|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# les 10 derniers mots différents (distinct = .unique()) lorsque classés par ordre alphabétique\n",
    "iliad.distinct() \\\n",
    "    .orderBy(\"word\", ascending=False) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP avec l'Iliade et l'Odyssé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But : A partir d'une ligne de texte , prédire si elle appartient à l'Iliade ou l'Odyssé.\n",
    "\n",
    "Algo : classification supervisée binaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture de l'Iliade en lignes\n",
    "iliad_lines = spark.sparkContext.textFile(\"iliad.mb.txt\") \\\n",
    "        .map(lambda line: line.split()) \\\n",
    "        .map(lambda words: [w.strip(\",;.?:/!\\\"'~()-\") for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'Antilochus', 'speared', 'Atymnius', 'driving', 'the', 'point', 'of', 'the', 'spear']\n",
      "['for', 'you', 'are', 'foremost', 'at', 'all', 'times', 'alike', 'in', 'fight', 'and', 'counsel', 'hold']\n",
      "['go', 'to', 'Ilius', 'and', 'tell', 'the', 'old', 'men', 'of', 'our', 'council', 'and', 'our', 'wives', 'to', 'pray']\n",
      "['scare', 'me', 'as', 'though', 'I', 'were', 'a', 'child', 'I', 'too', 'if', 'I', 'will', 'can', 'brag', 'and']\n",
      "['half', 'true', 'and', 'the', 'other', 'lies', 'as', 'rage', 'inspires', 'them', 'No', 'words', 'of', 'yours']\n"
     ]
    }
   ],
   "source": [
    "for line in iliad_lines.takeSample(False, 5):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idem pour l'Odyssé\n",
    "odyssey_lines = spark.sparkContext.textFile(\"odyssey.mb.txt\") \\\n",
    "        .map(lambda line: line.split()) \\\n",
    "        .map(lambda words: [w.strip(\",;.?:/!\\\"'~()-\") for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labéliser nos lignes (0 = Iliade, 1 = Odyssé) et transformer en rows\n",
    "iliad_labeled = iliad_lines.map(lambda words: pyspark.sql.Row(label=0, words=words))\n",
    "odyssey_labeled = odyssey_lines.map(lambda words: pyspark.sql.Row(label=1, words=words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union des rdd labélisés\n",
    "data = spark.createDataFrame(iliad_labeled.union(odyssey_labeled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va faire un bag of word de nos mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = feature.CountVectorizer(inputCol=\"words\", outputCol=\"bow\") \\\n",
    "    .fit(data)\n",
    "features = vectorizer.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|               words|                 bow|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[Provided, by, Th...|(10915,[35,72,370...|\n",
      "|    0|[See, bottom, for...|(10915,[12,36,104...|\n",
      "|    0|[http://classics....|(10915,[9874],[1.0])|\n",
      "|    0|                  []|       (10915,[],[])|\n",
      "|    0|        [The, Iliad]|(10915,[72,10762]...|\n",
      "|    0|         [By, Homer]|(10915,[1613,6680...|\n",
      "|    0|                  []|       (10915,[],[])|\n",
      "|    0|                  []|       (10915,[],[])|\n",
      "|    0|[Translated, by, ...|(10915,[35,5188,5...|\n",
      "|    0|                  []|       (10915,[],[])|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation en train et test\n",
    "train, test = features.randomSplit([0.75, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On prend un classifieur Naive Bayes et on le fit sur le train\n",
    "clf = classification.NaiveBayes(labelCol=\"label\", featuresCol=\"bow\", predictionCol=\"target_predicted\") \\\n",
    "    .fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict sur le test\n",
    "predicted = clf.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7770646850194224\n"
     ]
    }
   ],
   "source": [
    "# Evaluation avec l'accuracy\n",
    "accuracy = predicted.filter(predicted.target_predicted==predicted.label).count() / float(predicted.count())\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.78\n"
     ]
    }
   ],
   "source": [
    "# ou encore\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"target_predicted\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predicted)\n",
    "\n",
    "print(f\"Accuracy = {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on spark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupérer les valeurs et ajouter 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df.select(\"age\").collect():\n",
    "    print(row[0] +1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Récupérer les données sous forme de Row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = df.rdd.map(lambda loop: (loop[\"id\"], loop[\"age\"]))\n",
    "rdd.toDF([\"ID\", \"NAME\"]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appliquer une fonction modifiant les clonnes du df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('James','Smith','M',30),('Anna','Rose','F',41),\n",
    "  ('Robert','Williams','M',62), \n",
    "]\n",
    "columns = [\"firstname\",\"lastname\",\"gender\",\"salary\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(x):\n",
    "    firstName=x.firstname\n",
    "    name=firstName+\"-ok\"\n",
    "    gender=x.gender.lower()\n",
    "    salary=x.salary*2\n",
    "    return (name,gender,salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2=df.rdd.map(lambda x: func1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = rdd2.toDF([\"name\", \"gender\", \"salary\"])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajouter \"-done\" à la fin du texte target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_txt(row):\n",
    "    txt = row.target\n",
    "    new_txt = txt + \"-done\"\n",
    "    return (row.path, row.content, new_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = img_df.rdd.map(lambda row: modify_txt(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = res.toDF([\"path\", \"content\", \"target\"])\n",
    "res_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créer une colonne avec les images converties en vecteurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec udf :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_simple(img_bytes):\n",
    "    img_example = Image.open(io.BytesIO(img_bytes))\n",
    "    img_example = np.array(img_example)\n",
    "    return img_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro_udf = udf(lambda x: prepro_simple(x), ArrayType(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ne marche pas\n",
    "img_df.select(\"path\", \"target\", prepro_udf(\"content\").alias(\"prepro\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec pandas_udf en décorateur :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"long\")\n",
    "def preprocess_img(img_col: Iterator[BinaryIO]) -> Iterator[ArrayType(FloatType())]:\n",
    "    \"\"\"\n",
    "    Convert a bytes image to numpy array.\n",
    "    Resize and add a border to keep aspect ratio.\n",
    "    Apply ResNet50 preprocessing.\n",
    "    \"\"\"\n",
    "    border_color = (255, 255, 255)\n",
    "    final_size = (224, 224)\n",
    "\n",
    "    for bytes_img in img_col:\n",
    "        # Convert to numpy array and get width and height\n",
    "        img = Image.open(io.BytesIO(bytes_img))\n",
    "        width, height = img.size\n",
    "        img_array = np.array(img)\n",
    "\n",
    "        # Resize and add border if necessary\n",
    "        if width == height:\n",
    "            img_border=img_array\n",
    "        elif height > width:\n",
    "            border = round((height - width) / 2)\n",
    "            img_border = cv2.copyMakeBorder(img_array, 0, 0, border, border, \n",
    "                                            cv2.BORDER_CONSTANT, None, value = border_color)\n",
    "        else:\n",
    "            border = round((width - height) / 2)\n",
    "            img_border = cv2.copyMakeBorder(img_array, border, border, 0, 0, \n",
    "                                            cv2.BORDER_CONSTANT, None, value = border_color)\n",
    "        img_resized = cv2.resize(img_border, final_size, interpolation = cv2.INTER_AREA)\n",
    "\n",
    "        # Resnet50 preprocessing\n",
    "        img_preprocessed = resnet50.preprocess_input(img_resized)\n",
    "\n",
    "        yield img_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ne marche pas\n",
    "preprocessed_df = img_df.select(col(\"path\"), col(\"target\"), preprocess_img(\"content\").alias(\"preprocessed\"))\n",
    "preprocessed_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec pandas_udf :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twice_func(age: pd.Series) -> pd.Series:\n",
    "    return age*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twice = pandas_udf(twice_func, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(twice(col(\"age\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_simple(iterator):\n",
    "    for img_bytes in iterator:\n",
    "        img_example = Image.open(io.BytesIO(img_bytes))\n",
    "        img_example = np.array(img_example)\n",
    "    yield img_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "caf1c2fcf97217de91eafa76b907d50f9ea378f5ffbee7f571142d119bb6a771"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
